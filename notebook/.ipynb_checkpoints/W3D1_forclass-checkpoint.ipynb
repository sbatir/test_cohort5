{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Week 3, Day 1\n",
    "\n",
    "Hi class! Today we'll be using the Jupyter notebook to help you visualize some code and directly \n",
    "interact with your environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we import the necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that you can import the filename with no errors. \n",
    "Then highlight on the cell below and click cmd + enter or ctrl + enter, depending on your OS. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Data Exploration ( 40 minutes)\n",
    "\n",
    "You will be using this in groups of 2. The objective is for you to apply one of our algorithms to perform \"exploratory data analysis\"\n",
    "on this diabetes csv file. You can find the corresponding .csv in the same directory as this folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Final WhiteBoarding ( 50 minutes) \n",
    "#(I've done the pre-processing for you!) You may use this in a code block and then \n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "filename = 'diabetes.csv'\n",
    "names = ['pregnancies', 'glucose', 'bp', 'skinthick', 'insulin', 'bmi', 'diabetes', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names, header=0, index_col=False)\n",
    "#For class label \n",
    "\n",
    "array = dataframe.values\n",
    "#print(array[:,0:8])\n",
    "X = array[:,0:8]\n",
    "print(X.shape)\n",
    "#print(X)\n",
    "Y = array[:,8]\n",
    "#print(Y)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filename = 'iris.data.csv'\n",
    "dataframe = read_csv(filename, sep=',', delim_whitespace=True)\n",
    "array = dataframe.values\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the shape for sanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import K Folds\n",
    "from numpy import array\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# data sample\n",
    "data = array([0.10, 0.21, 0.32, 0.43, 0.54, 0.65, 0.76, 0.87, 0.98, 1.0])\n",
    "# prepare cross validation\n",
    "kfold = KFold(6, True, 2)\n",
    "\n",
    "# Print out each possible split. \n",
    "for train, test in kfold.split(data):\n",
    "\tprint('train: %s, test: %s' % (data[train], data[test]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Example - Official Documentation from SKLearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://scikit-learn.org/stable/auto_examples/tree/plot_iris_dtc.html#sphx-glr-auto-examples-tree-plot-iris-dtc-py\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "# Parameters\n",
    "n_classes = 3\n",
    "plot_colors = \"ryb\"\n",
    "plot_step = 0.02\n",
    "\n",
    "# Load data\n",
    "iris = load_iris()\n",
    "\n",
    "for pairidx, pair in enumerate([[0, 1], [0, 2], [0, 3],\n",
    "                                [1, 2], [1, 3], [2, 3]]):\n",
    "    # We only take the two corresponding features\n",
    "    X = iris.data[:, pair]\n",
    "    y = iris.target\n",
    "\n",
    "    # Train\n",
    "    clf = DecisionTreeClassifier().fit(X, y)\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    plt.subplot(2, 3, pairidx + 1)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                         np.arange(y_min, y_max, plot_step))\n",
    "    plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)\n",
    "\n",
    "    plt.xlabel(iris.feature_names[pair[0]])\n",
    "    plt.ylabel(iris.feature_names[pair[1]])\n",
    "\n",
    "    # Plot the training points\n",
    "    for i, color in zip(range(n_classes), plot_colors):\n",
    "        idx = np.where(y == i)\n",
    "        plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i],\n",
    "                    cmap=plt.cm.RdYlBu, edgecolor='black', s=15)\n",
    "\n",
    "plt.suptitle(\"Decision surface of a decision tree using paired features\")\n",
    "plt.legend(loc='lower right', borderpad=0, handletextpad=0)\n",
    "plt.axis(\"tight\")\n",
    "\n",
    "plt.figure()\n",
    "clf = DecisionTreeClassifier().fit(iris.data, iris.target)\n",
    "plot_tree(clf, filled=True)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot The Tree\n",
    "\n",
    "The following code block below will illustrate how we save the iris dataset to a classifier (clf), followed by an opportunity for you to see the output of a decision tree and observe how we converge to leaves with a final gini value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the Iris Example for Decision tree#\n",
    "from sklearn.datasets import load_iris \n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(random_state=0)\n",
    "iris = load_iris()\n",
    "clf = clf.fit(iris.data, iris.target)\n",
    "tree.plot_tree(clf)  # doctest: +SKIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Adventure Into a Random Forest\n",
    "\n",
    "The following block is an example to demonstrate the power of random forests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Code Block \n",
    "# Random Forest Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "filename = 'iris.data.csv'\n",
    "names = ['feature1', 'feature2', 'feature3', 'feature4']\n",
    "dataframe = read_csv(filename, names=names, index_col=False)\n",
    "#For class label \n",
    "dataframe2 = read_csv(filename, names=names)\n",
    "\n",
    "array = dataframe.values\n",
    "array2 = dataframe2.values\n",
    "#print(array[:,0:4])\n",
    "X = array[:,0:5]\n",
    "#print(X)\n",
    "Y = array2[:,3]\n",
    "#print(Y)\n",
    "num_trees = 100\n",
    "max_features = 3\n",
    "kfold = KFold(n_splits=5, random_state=3)\n",
    "model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Information from Sci-Kit Learn \n",
    "A comparison of several classifiers, we care primary about randomforest and decisiontree classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Source: https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_iris.html#sphx-glr-auto-examples-ensemble-plot-forest-iris-py\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n",
    "                              AdaBoostClassifier)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Parameters\n",
    "n_classes = 3\n",
    "n_estimators = 30\n",
    "cmap = plt.cm.RdYlBu\n",
    "plot_step = 0.02  # fine step width for decision surface contours\n",
    "plot_step_coarser = 0.5  # step widths for coarse classifier guesses\n",
    "RANDOM_SEED = 13  # fix the seed on each iteration\n",
    "\n",
    "# Load data\n",
    "iris = load_iris()\n",
    "\n",
    "plot_idx = 1\n",
    "\n",
    "models = [DecisionTreeClassifier(max_depth=None),\n",
    "          RandomForestClassifier(n_estimators=n_estimators),\n",
    "          ExtraTreesClassifier(n_estimators=n_estimators),\n",
    "          AdaBoostClassifier(DecisionTreeClassifier(max_depth=3),\n",
    "                             n_estimators=n_estimators)]\n",
    "\n",
    "for pair in ([0, 1], [0, 2], [2, 3]):\n",
    "    for model in models:\n",
    "        # We only take the two corresponding features\n",
    "        X = iris.data[:, pair]\n",
    "        y = iris.target\n",
    "\n",
    "        # Shuffle\n",
    "        idx = np.arange(X.shape[0])\n",
    "        np.random.seed(RANDOM_SEED)\n",
    "        np.random.shuffle(idx)\n",
    "        X = X[idx]\n",
    "        y = y[idx]\n",
    "\n",
    "        # Standardize\n",
    "        mean = X.mean(axis=0)\n",
    "        std = X.std(axis=0)\n",
    "        X = (X - mean) / std\n",
    "\n",
    "        # Train\n",
    "        model.fit(X, y)\n",
    "\n",
    "        scores = model.score(X, y)\n",
    "        # Create a title for each column and the console by using str() and\n",
    "        # slicing away useless parts of the string\n",
    "        model_title = str(type(model)).split(\n",
    "            \".\")[-1][:-2][:-len(\"Classifier\")]\n",
    "\n",
    "        model_details = model_title\n",
    "        if hasattr(model, \"estimators_\"):\n",
    "            model_details += \" with {} estimators\".format(\n",
    "                len(model.estimators_))\n",
    "        print(model_details + \" with features\", pair,\n",
    "              \"has a score of\", scores)\n",
    "\n",
    "        plt.subplot(3, 4, plot_idx)\n",
    "        if plot_idx <= len(models):\n",
    "            # Add a title at the top of each column\n",
    "            plt.title(model_title, fontsize=9)\n",
    "\n",
    "        # Now plot the decision boundary using a fine mesh as input to a\n",
    "        # filled contour plot\n",
    "        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                             np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "        # Plot either a single DecisionTreeClassifier or alpha blend the\n",
    "        # decision surfaces of the ensemble of classifiers\n",
    "        if isinstance(model, DecisionTreeClassifier):\n",
    "            Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "            Z = Z.reshape(xx.shape)\n",
    "            cs = plt.contourf(xx, yy, Z, cmap=cmap)\n",
    "        else:\n",
    "            # Choose alpha blend level with respect to the number\n",
    "            # of estimators\n",
    "            # that are in use (noting that AdaBoost can use fewer estimators\n",
    "            # than its maximum if it achieves a good enough fit early on)\n",
    "            estimator_alpha = 1.0 / len(model.estimators_)\n",
    "            for tree in model.estimators_:\n",
    "                Z = tree.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "                Z = Z.reshape(xx.shape)\n",
    "                cs = plt.contourf(xx, yy, Z, alpha=estimator_alpha, cmap=cmap)\n",
    "\n",
    "        # Build a coarser grid to plot a set of ensemble classifications\n",
    "        # to show how these are different to what we see in the decision\n",
    "        # surfaces. These points are regularly space and do not have a\n",
    "        # black outline\n",
    "        xx_coarser, yy_coarser = np.meshgrid(\n",
    "            np.arange(x_min, x_max, plot_step_coarser),\n",
    "            np.arange(y_min, y_max, plot_step_coarser))\n",
    "        Z_points_coarser = model.predict(np.c_[xx_coarser.ravel(),\n",
    "                                         yy_coarser.ravel()]\n",
    "                                         ).reshape(xx_coarser.shape)\n",
    "        cs_points = plt.scatter(xx_coarser, yy_coarser, s=15,\n",
    "                                c=Z_points_coarser, cmap=cmap,\n",
    "                                edgecolors=\"none\")\n",
    "\n",
    "        # Plot the training points, these are clustered together and have a\n",
    "        # black outline\n",
    "        plt.scatter(X[:, 0], X[:, 1], c=y,\n",
    "                    cmap=ListedColormap(['r', 'y', 'b']),\n",
    "                    edgecolor='k', s=20)\n",
    "        plot_idx += 1  # move on to the next plot in sequence\n",
    "\n",
    "plt.suptitle(\"Classifiers on feature subsets of the Iris dataset\", fontsize=12)\n",
    "plt.axis(\"tight\")\n",
    "plt.tight_layout(h_pad=0.2, w_pad=0.2, pad=2.5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
